---
title: "Ensembles - Conceptos Básicos"
author: "Miguel Conde"
date: "20 de febrero de 2017"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.align = "center")
```

Cuando en aprendizaje máquina hablamos de *ensembles* nos referimos a **modelos de modelos**.

La aproximación básica para crear un modelo de clasificación o regresión es construirlo a partir de unos datos de entrenamiento (*training data*) que contienen tanto la *variable objetivo* como las *variables predictoras*. Un modelo así construido nos permite entonce *predecir* la variable objetivo a partir de las variables predictoras de cualquier otro conjunto de datos estructurado como el de entrenamiento.

Esto es lo que hemos visto, por ejemplo, en los posts *Arboles de Decisión* [I](http://es100x100datascience.com/arboles-de-decision-i/), [II](http://es100x100datascience.com/arboles-de-decision-ii/), [III](http://es100x100datascience.com/arboles-de-decision-iii/) y [IV](http://es100x100datascience.com/arboles-de-decision-iv/), en los que construimos modelos de clasificación basados en algoritmos tipo árboles de decisión para predecir la tasa de rotación de los clientes de una operadora.

Podemos dar un paso más en la construcción de modelos de aprendizaje máquina mediante la técnica conocida como **ensemble**. Hemos dicho arriba que se trata de modelos de modelos: en efecto, se trata de modelos construidos no directamente a partir de los datos de un conjunto de datos de entrenamiento, sino  a partir de las predicciones de **varios** modelos construidos - estos si - a partir de los datos de entrenamiento.

Es decir, primero entrenamos unos cuantos modelos tradicionales (modelos de primer nivel); y, a continuación, a partir de las predicciones de los modelos de primer nivel construimos un  segundo modelo (modelo de segundo nivel).

Lo que estamos intentando con esto es construir un modelo "fuerte" a partir de otros modelos más "débiles". Es una estrategia de juego en equipo: si logramos conjuntar el "juego" de varios modelos, quizá podamos construir un equipo más "fuerte" que cualquiera de sus componentes. 

Claro, para conseguir esto los componentes "débiles" deben estar especializados en diferentes cosas (por ejemplo, unos serán defensas, otros delanteros y otros centrocampistas) todas ellas neecsarias para que el equipo así obtenido sea más potente que sus componentes por separado. Esto último significa que los modelos que ompongan el *ensemble* deben ser distintos entre sí (baja correlación entre sus predicciones).

Por ejemplo, podríamos utilizar, como modelos de primer nivel, un *random  forest*, una *máquina de vector soporte* y un *extreme gradient boosting*; y, como modelo de segundo nivel, una *red neuronal*.

Caben todo tipo de aproximaciones: podría haber muchos más modelos en el primer nivel, incluso del mismo tipo pero con diferentes parámetros. Y el modelo de segundo nivel podría ser tan sencillo como una **media**, una **media ponderada** o un **recuento** tipo "votación en la que la mayoría gana" de las predicciones de los modelos de primer nivel. Si el *ensemble* utiliza como modelo de segundo nivel uno complejo, basado en un algoritmo de aprendizaje máquina como *random forest*, *redes neuronales*, *support vector machines*, etc., entonces se denomina **stack** ("pila") de modelos, y la técnica asociada se denomina **stacking**.

Los *ensembles* suelen mejorar el rendimiento de los modelos de primer nivel, sobre todo si el de estos últimos no es espectacular. 

Como ejemplo, vamos a construir algunos ensembles para el mismo problema de los posts *Arboles de Decisión*. [I](http://es100x100datascience.com/arboles-de-decision-i/), [II](http://es100x100datascience.com/arboles-de-decision-ii/), [III](http://es100x100datascience.com/arboles-de-decision-iii/) y [IV](http://es100x100datascience.com/arboles-de-decision-iv/)

En primer lugar, vamos a cargar los datos:

```{r}
library(C50)
data(churn)
```

No vamos a repetir aquí la exploración que ya hemos hecho en los posts referidos, sino que vamos a ir directamente a la construcción del *ensemble*. Para construir los modelos vamos a utilizar el paquete [`caret`](https://cran.r-project.org/web/packages/caret/index.html).

Preparemos los datos y dividámoslos en un *train set* y un *validation set*:
```{r}
churn <- rbind(churnTrain, churnTest)

# Variables target y predictoras (features)
target      <- "churn"
predictoras <- names(churn)[names(churn) != target]

# Convertimos factors a integer para que no nos de problemas con svm ni xgbm
for (v in predictoras) {
  if (is.factor(churn[, v])) {
    newName <- paste0("F_", v)
    names(churn)[which(names(churn) == v)] <- newName
    churn[, v] <-  unclass(churn[, newName])
  }
}

library(caret)
set.seed(123)
train_idx   <- createDataPartition(churn$churn, p = 0.75, list = FALSE)
churn_train <- churn[ train_idx,]
churn_valid <- churn[-train_idx,]
```

Preparemos ahora los controles que vamos a utilizar al construir nuestros modelos:
```{r}
trControl <- trainControl(
                          # 5-fold Cross Validation
                          method = "cv", 
                          number = 5,
                          # Save the predictions for the optimal tuning 
                          # parameters
                          savePredictions = 'final', 
                          # Class probabilities will be computed along with
                          # predicted values in each resample
                          classProbs = TRUE
                         ) 
```

Ya que lo hemos mencionado antes, vamos a entrenar un `random forest`, un `svm` tipo *radial* y un `xgbm` tipo "tree* como modelos de primer nivel.
```{r}
model_rf   <- train(churn_train[ , predictoras], churn_train[ , target],
                    method     = "rf",
                    trControl  = trControl,
                    tuneLength = 3)
model_svm  <- train(churn_train[ , predictoras], churn_train[ , target],
                    method     = "svmRadial",
                    trControl  = trControl,
                    tuneLength = 3)
model_xgbm <- train(churn_train[ , predictoras], churn_train[ , target],
                    method     = "xgbTree",
                    trControl  = trControl,
                    tuneLength = 3)
```

Veamos la performance de los 3 modelos y comparemos (nótese que las medidas de rendimiento se toman mediante *Cross validation* sobre el *training set*):

```{r}
resamps <- resamples(list(rf = model_rf, svm = model_svm, xgbm = model_xgbm))
summary(resamps)

bwplot(resamps)
```

Los tres modelos presentan una elevada *accuracy*, aunque la *kappa* del svm es notablemente menor que la de los otros dos.

```{r}
diffs <- diff(resamps)
summary(diffs)
```
Además xgbm y rf dan resultados muy correlados.

A partir de esto podríamos quedarnos con svm y elegir entre xgbm y rf para a continuación tratar de añadir más modelos poco correlados con los dos elegidos.

Sin embargo, vamos a darnos por satisfechos con los tres modelos para continuar con el ejemplo y construir algunos modelos de nivel 2.

Lo primero que necesito son las nuevas variables predictoras, esta vez de segundo nivel:

```{r}
# Utilizamos los modelos de 1er nivel para predecir 
churn_valid$pred_rf   <- predict(object = model_rf, 
                                 churn_valid[ , predictoras])
churn_valid$pred_svm  <- predict(object = model_svm, 
                                 churn_valid[ , predictoras])
churn_valid$pred_xgbm <- predict(object = model_xgbm, 
                                 churn_valid[ , predictoras])

# Y sus probabilidades
churn_valid$pred_rf_prob   <- predict(object = model_rf,
                                      churn_valid[,predictoras],
                                      type='prob')
churn_valid$pred_svm_prob  <- predict(object = model_svm,
                                      churn_valid[,predictoras],
                                      type='prob')
churn_valid$pred_xgbm_prob <- predict(object = model_xgbm,
                                      churn_valid[,predictoras],
                                      type='prob')
```

Empecemos con una simple media:

```{r}
## PROMEDIO
# Calculamos la media de las predictoras de primer nivel
churn_valid$pred_avg <- (churn_valid$pred_rf_prob$yes +
                           churn_valid$pred_svm_prob$yes +
                           churn_valid$pred_xgbm_prob$yes) / 3

# Dividimos las clases binarias en p = 0.5
churn_valid$pred_avg <- as.factor(ifelse(churn_valid$pred_avg > 0.5, 
                                         'yes', 'no'))
```

Ahora la media ponderada. Como el orden de los modelos de primer nivel, según su *Accuracy*, era rf y xgbm (empatados) seguidos por svm, vamos a asignarle pesos 0.25, 0.25 y 0.5:
```{r}
## MEDIA PONDERADA
# Calculamos la media ponderada de las predictoras de primer nivel
churn_valid$pred_weighted_avg <- (churn_valid$pred_rf_prob$yes * 0.25) +
  (churn_valid$pred_xgbm_prob$yes * 0.25) + 
  (churn_valid$pred_svm_prob$yes * 0.5)

# Dividimos las clases binarias en p = 0.5
churn_valid$pred_weighted_avg <- as.factor(ifelse(churn_valid$pred_weighted_avg > 0.5, 
                                              'yes', 'no'))
```

Por último, hagamos que los modelos "voten":
```{r}
## VOTACIÓN
# La mayoría gana
predictoras2N <- c("pred_rf", "pred_xgbm", "pred_svm")
churn_valid$pred_majority <- 
  as.factor(apply(churn_valid[, predictoras2N],
                  1, 
                  function(x) {
                    if (sum(x == "yes") > sum(x == "no"))
                      return("yes")
                    else
                      return("no")
                    }))
```

Comparemos resultados contra el *test set*:
```{r}
## PROMEDIO
confusionMatrix(churn_valid$churn, churn_valid$pred_avg)
```

```{r}
## MEDIA PONDERADA
confusionMatrix(churn_valid$churn, churn_valid$pred_weighted_avg)
```

```{r}
## ## VOTACIÓN
confusionMatrix(churn_valid$churn, churn_valid$pred_majority)
```


Como se ve, los 3 modelos de segundo nivel dan resultados ligeramente mejores que los de primer nivel.