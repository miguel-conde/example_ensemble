---
title: "Ensembles - Conceptos Básicos"
author: "Miguel Conde"
date: "20 de febrero de 2017"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.align = "center")
```

Cuando en aprendizaje máquina hablamos de *ensembles* nos referimos a **modelos de modelos**.

La aproximación básica para crear un modelo de clasificación o regresión es construirlo a partir de unos datos de entrenamiento (*training data*) que contienen tanto la *variable objetivo* como las *variables predictoras*. Un modelo así construido nos permite entonce *predecir* la variable objetivo a partir de las variables predictoras de cualquier otro conjunto de datos estructurado como el de entrenamiento.

Esto es lo que hemos visto, por ejemplo, en los posts *Arboles de Decisión* [I](http://es100x100datascience.com/arboles-de-decision-i/), [II](http://es100x100datascience.com/arboles-de-decision-ii/), [III](http://es100x100datascience.com/arboles-de-decision-iii/) y [IV](http://es100x100datascience.com/arboles-de-decision-iv/), en los que construimos modelos de clasificación basados en algoritmos tipo árboles de decisión para predecir la tasa de rotación de los clientes de una operadora.

Podemos dar un paso más en la construcción de modelos de aprendizaje máquina mediante la técnica conocida como **ensemble**. Hemos dicho arriba que se trata de modelos de modelos: en efecto, se trata de modelos construidos no directamente a partir de los datos de un conjunto de datos de entrenamiento, sino  a partir de las predicciones de **varios** modelos construidos - estos si - a partir de los datos de entrenamiento.

Es decir, primero entrenamos unos cuantos modelos tradicionales (modelos de primer nivel); y, a continuación, a partir de las predicciones de los modelos de primer nivel construimos un  segundo modelo (modelo de segundo nivel).

Lo que estamos intentando con esto es construir un modelo "fuerte" a partir de otros modelos más "débiles". Es una estrategia de juego en equipo: si logramos conjuntar el "juego" de varios modelos, quizá podamos construir un equipo más "fuerte" que cualquiera de sus componentes. 

Claro, para conseguir esto los componentes "débiles" deben estar especializados en diferentes cosas (por ejemplo, unos serán defensas, otros delanteros y otros centrocampistas) todas ellas neecsarias para que el equipo así obtenido sea más potente que sus componentes por separado. Esto último significa que los modelos que ompongan el *ensemble* deben ser distintos entre sí (baja correlación entre sus predicciones).

Por ejemplo, podríamos utilizar, como modelos de primer nivel, un *random  forest*, una *máquina de vector soporte* y un *extreme gradient boosting*; y, como modelo de segundo nivel, una *red neuronal*.

Caben todo tipo de aproximaciones: podría haber muchos más modelos en el primer nivel, incluso del mismo tipo pero con diferentes parámetros. Y el modelo de segundo nivel podría ser tan sencillo como una **media**, una **media ponderada** o un **recuento** tipo "votación en la que la mayoría gana" de las predicciones de los modelos de primer nivel. Si el *ensemble* utiliza como modelo de segundo nivel uno complejo, basado en un algoritmo de aprendizaje máquina como *random forest*, *redes neuronales*, *support vector machines*, etc., entonces se denomina **stack** ("pila") de modelos, y la técnica asociada se denomina **stacking**.

Entre los *ensembles* hay que destacar también dos técnicas avanzadas, el *bagging* y el *boosting*. Más adelante, en otros posts, hablaremos de ellas.

Los *ensembles* suelen mejorar el rendimiento de los modelos de primer nivel, sobre todo si el de estos últimos no es espectacular. 

Como ejemplo, vamos a construir algunos ensembles para el mismo problema de los posts *Arboles de Decisión*. [I](http://es100x100datascience.com/arboles-de-decision-i/), [II](http://es100x100datascience.com/arboles-de-decision-ii/), [III](http://es100x100datascience.com/arboles-de-decision-iii/) y [IV](http://es100x100datascience.com/arboles-de-decision-iv/)

